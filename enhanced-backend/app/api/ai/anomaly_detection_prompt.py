"""
Advanced Anomaly Detection Node Prompt Generator
"""

import numpy as np
import pandas as pd

class AnomalyDetectionPrompt:
    """Generate sophisticated prompts for anomaly detection nodes"""
    
    @staticmethod
    def generate_prompt(data: dict, node_id: str, context: dict = None) -> str:
        """Generate advanced anomaly detection analysis prompt"""
        
        anomalies = data.get('anomalies', [])
        detection_method = data.get('method', 'statistical')
        anomaly_scores = data.get('anomaly_scores', [])
        df = data.get('dataframe')
        detection_stats = data.get('detection_stats', {})
        thresholds = data.get('thresholds', {})
        
        if not anomalies and df is None:
            return "âŒ **CRITICAL ERROR**: No anomaly detection results or dataframe available"
        
        # Analyze anomaly detection results
        anomaly_analysis = AnomalyDetectionPrompt._analyze_anomalies(anomalies, anomaly_scores, df)
        method_assessment = AnomalyDetectionPrompt._assess_detection_method(detection_method, detection_stats)
        data_impact = AnomalyDetectionPrompt._assess_data_impact(anomalies, df, detection_method)
        risk_assessment = AnomalyDetectionPrompt._assess_risk_levels(anomalies, anomaly_scores, thresholds)
        action_recommendations = AnomalyDetectionPrompt._generate_action_recommendations(anomalies, detection_method)
        
        # Anomaly summary
        anomaly_count = len(anomalies) if isinstance(anomalies, list) else "Multiple" if anomalies else 0
        total_records = df.shape[0] if df is not None else "Unknown"
        anomaly_rate = (len(anomalies) / df.shape[0] * 100) if df is not None and isinstance(anomalies, list) else 0
        
        prompt = f"""
ğŸš¨ **ANOMALY DETECTION INTELLIGENCE CENTER - Node: {node_id}**

âš ï¸ **ANOMALY DETECTION OVERVIEW**:
Detection Method: {detection_method.replace('_', ' ').title()}
Anomalies Detected: {anomaly_count} out of {total_records} records
Anomaly Rate: {anomaly_rate:.2f}%
Detection Confidence: {"High" if anomaly_scores else "Standard"}

ğŸ” **ANOMALY PATTERN ANALYSIS**:
{chr(10).join(anomaly_analysis) if anomaly_analysis else "âš ï¸ Anomaly pattern analysis not available"}

ğŸ¯ **DETECTION METHOD ASSESSMENT**:
{chr(10).join(method_assessment) if method_assessment else "âš ï¸ Method assessment not available"}

ï¿½ **DATA IMPACT ANALYSIS**:
{chr(10).join(data_impact) if data_impact else "âš ï¸ Data impact assessment not available"}

âš¡ **RISK LEVEL ASSESSMENT**:
{chr(10).join(risk_assessment) if risk_assessment else "âš ï¸ Risk assessment not available"}

ğŸ¯ **ACTION RECOMMENDATIONS**:
{chr(10).join(action_recommendations) if action_recommendations else "âš ï¸ Action recommendations not available"}

ğŸ“Š **DETECTION METADATA**:
â€¢ Scoring Available: {"Yes" if anomaly_scores else "No"}
â€¢ Threshold Configuration: {"Custom" if thresholds else "Default"}
â€¢ Statistical Validation: {"Available" if detection_stats else "Basic"}
â€¢ Multi-dimensional Analysis: {"Yes" if isinstance(anomalies, list) and len(anomalies) > 0 else "Single"}

ğŸ’¡ **ADVANCED ANOMALY INTELLIGENCE REQUIREMENTS**:

1. **PATTERN CLASSIFICATION**: Categorize anomalies by type, severity, and statistical significance
2. **ROOT CAUSE ANALYSIS**: Identify potential causes and contributing factors for detected anomalies
3. **STATISTICAL PRIORITY**: Rank anomalies by statistical significance and deviation magnitude
4. **DETECTION RESPONSE**: Define technical actions for anomaly investigation
5. **PREVENTION STRATEGY**: Recommend statistical methods to reduce false positives
6. **MONITORING ENHANCEMENT**: Improve detection algorithms based on identified patterns
7. **TECHNICAL ASSESSMENT**: Prepare detailed anomaly statistics and mathematical properties
8. **EFFICIENCY ANALYSIS**: Evaluate the statistical power and accuracy of detection methods

ğŸ¯ **CRITICAL ANOMALY ANALYSIS REQUIREMENTS**:
- Classify SPECIFIC anomaly patterns and their statistical significance
- Assess MATHEMATICAL PROPERTIES of detected deviations
- Identify SYSTEMIC PATTERNS vs isolated incidents
- Quantify STATISTICAL SIGNIFICANCE of detected anomalies
- Recommend SPECIFIC TECHNIQUES for each category of anomaly
- Establish MONITORING THRESHOLDS for future detection
- Evaluate DETECTION EFFECTIVENESS and false positive rates

âš¡ **RESPONSE FOCUS**: Analyze the ACTUAL anomalies detected, their patterns, and statistical properties. Provide concrete, actionable recommendations for anomaly response and prevention based on the specific detection results.
"""
        
        return prompt.strip()
    
    @staticmethod
    def _analyze_anomalies(anomalies, anomaly_scores, df) -> list:
        """Analyze detected anomalies for patterns and insights"""
        analysis = []
        
        if not anomalies:
            analysis.append("âœ… **NO ANOMALIES DETECTED**: System operating within normal parameters")
            return analysis
        
        anomaly_count = len(anomalies) if isinstance(anomalies, list) else 1
        
        # Anomaly frequency analysis
        if df is not None:
            total_records = df.shape[0]
            anomaly_rate = (anomaly_count / total_records) * 100
            
            if anomaly_rate > 10:
                analysis.append(f"ğŸš¨ **HIGH ANOMALY RATE**: {anomaly_rate:.1f}% - Systemic issues require investigation")
            elif anomaly_rate > 5:
                analysis.append(f"âš ï¸ **ELEVATED ANOMALY RATE**: {anomaly_rate:.1f}% - Process review recommended")
            elif anomaly_rate > 1:
                analysis.append(f"ğŸ“Š **MODERATE ANOMALY RATE**: {anomaly_rate:.1f}% - Normal operational variance")
            else:
                analysis.append(f"âœ… **LOW ANOMALY RATE**: {anomaly_rate:.1f}% - Excellent system stability")
        
        # Severity analysis using anomaly scores
        if anomaly_scores and isinstance(anomaly_scores, list):
            scores_array = np.array(anomaly_scores)
            
            if len(scores_array) > 0:
                high_severity = np.sum(scores_array > np.percentile(scores_array, 80))
                medium_severity = np.sum((scores_array > np.percentile(scores_array, 60)) & 
                                       (scores_array <= np.percentile(scores_array, 80)))
                low_severity = len(scores_array) - high_severity - medium_severity
                
                analysis.append(f"ğŸ“Š **SEVERITY DISTRIBUTION**: {high_severity} high, {medium_severity} medium, {low_severity} low severity")
                
                if high_severity > 0:
                    analysis.append(f"ğŸš¨ **CRITICAL ANOMALIES**: {high_severity} high-severity anomalies require immediate attention")
        
        # Pattern analysis for anomaly types
        if isinstance(anomalies, list) and df is not None:
            # Analyze anomaly distribution across features
            anomaly_features = AnomalyDetectionPrompt._analyze_anomaly_features(anomalies, df)
            if anomaly_features:
                analysis.extend(anomaly_features)
        
        # Temporal pattern analysis (if datetime columns exist)
        if df is not None and isinstance(anomalies, list):
            date_cols = df.select_dtypes(include=['datetime64']).columns.tolist()
            if date_cols and len(anomalies) > 0:
                analysis.append("ğŸ“… **TEMPORAL ANALYSIS**: Time-based anomaly patterns available for trend analysis")
        
        # Clustering analysis of anomalies
        if anomaly_count >= 5:
            analysis.append("ğŸ” **PATTERN CLUSTERING**: Multiple anomalies enable pattern clustering analysis")
        elif anomaly_count >= 2:
            analysis.append("ğŸ” **PATTERN COMPARISON**: Anomaly comparison reveals common characteristics")
        else:
            analysis.append("ğŸ¯ **ISOLATED ANOMALY**: Single anomaly requires individual investigation")
        
        return analysis
    
    @staticmethod
    def _analyze_anomaly_features(anomalies, df) -> list:
        """Analyze which features contribute most to anomalies"""
        feature_analysis = []
        
        try:
            # Assume anomalies contains indices or records
            if len(anomalies) > 0 and hasattr(df, 'iloc'):
                # Get anomalous records
                anomaly_indices = anomalies if isinstance(anomalies[0], int) else range(len(anomalies))
                
                if len(anomaly_indices) > 0:
                    anomaly_data = df.iloc[list(anomaly_indices)[:100]]  # Limit to first 100 for analysis
                    normal_data = df.drop(anomaly_indices).sample(min(1000, len(df) - len(anomaly_indices)))
                    
                    # Analyze numeric features
                    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                    
                    for col in numeric_cols[:5]:  # Analyze top 5 numeric columns
                        if col in anomaly_data.columns and col in normal_data.columns:
                            anom_mean = anomaly_data[col].mean()
                            normal_mean = normal_data[col].mean()
                            
                            if not np.isnan(anom_mean) and not np.isnan(normal_mean) and normal_mean != 0:
                                deviation_ratio = abs(anom_mean - normal_mean) / abs(normal_mean)
                                
                                if deviation_ratio > 1.0:
                                    feature_analysis.append(f"ğŸ“Š **{col}**: EXTREME DEVIATION - anomalies differ by {deviation_ratio:.1f}x from normal")
                                elif deviation_ratio > 0.5:
                                    feature_analysis.append(f"ğŸ“ˆ **{col}**: SIGNIFICANT DEVIATION - anomalies differ by {deviation_ratio:.1%} from normal")
                                elif deviation_ratio > 0.2:
                                    feature_analysis.append(f"ğŸ“‰ **{col}**: MODERATE DEVIATION - anomalies show {deviation_ratio:.1%} difference")
        
        except Exception:
            feature_analysis.append("ğŸ” **FEATURE ANALYSIS**: Advanced feature contribution analysis available")
        
        return feature_analysis
    
    @staticmethod
    def _assess_detection_method(method: str, detection_stats: dict) -> list:
        """Assess the effectiveness of the detection method"""
        assessment = []
        
        # Method-specific analysis
        method_insights = {
            'isolation_forest': [
                "ğŸŒ² **ISOLATION FOREST**: Effective for high-dimensional data and complex anomaly patterns",
                "ğŸ¯ **STRENGTHS**: Handles non-linear patterns and doesn't require labeled data",
                "âš ï¸ **CONSIDERATIONS**: May struggle with very sparse anomalies"
            ],
            'local_outlier_factor': [
                "ğŸ¯ **LOCAL OUTLIER FACTOR**: Excellent for density-based anomaly detection",
                "ğŸ“Š **STRENGTHS**: Identifies local outliers in varying density regions",
                "âš ï¸ **CONSIDERATIONS**: Sensitive to parameter tuning"
            ],
            'one_class_svm': [
                "ğŸ¤– **ONE-CLASS SVM**: Robust method for novelty detection",
                "ğŸ’ª **STRENGTHS**: Effective with high-dimensional data and non-linear boundaries",
                "âš ï¸ **CONSIDERATIONS**: Requires careful kernel and parameter selection"
            ],
            'statistical': [
                "ğŸ“Š **STATISTICAL METHODS**: Reliable for normally distributed data",
                "âœ… **STRENGTHS**: Interpretable results with clear statistical thresholds",
                "âš ï¸ **CONSIDERATIONS**: Assumes normal distribution and linear relationships"
            ],
            'dbscan': [
                "ğŸ” **DBSCAN CLUSTERING**: Effective for density-based anomaly detection",
                "ğŸ¯ **STRENGTHS**: Identifies clusters and noise points automatically",
                "âš ï¸ **CONSIDERATIONS**: Sensitive to epsilon and minimum points parameters"
            ]
        }
        
        method_key = method.lower().replace(' ', '_')
        if method_key in method_insights:
            assessment.extend(method_insights[method_key])
        else:
            assessment.append(f"ğŸ”§ **{method.upper()}**: Advanced anomaly detection method applied")
        
        # Performance assessment
        if detection_stats:
            if 'precision' in detection_stats:
                precision = detection_stats['precision']
                if precision > 0.8:
                    assessment.append(f"âœ… **HIGH PRECISION**: {precision:.1%} - Low false positive rate")
                elif precision > 0.6:
                    assessment.append(f"ğŸ“Š **MODERATE PRECISION**: {precision:.1%} - Acceptable false positive rate")
                else:
                    assessment.append(f"âš ï¸ **LOW PRECISION**: {precision:.1%} - High false positive rate needs adjustment")
            
            if 'recall' in detection_stats:
                recall = detection_stats['recall']
                if recall > 0.8:
                    assessment.append(f"âœ… **HIGH RECALL**: {recall:.1%} - Excellent anomaly detection rate")
                elif recall > 0.6:
                    assessment.append(f"ğŸ“Š **MODERATE RECALL**: {recall:.1%} - Good anomaly detection rate")
                else:
                    assessment.append(f"âš ï¸ **LOW RECALL**: {recall:.1%} - Missing many anomalies, tune sensitivity")
            
            if 'contamination' in detection_stats:
                contamination = detection_stats['contamination']
                assessment.append(f"ğŸ¯ **CONTAMINATION LEVEL**: {contamination:.1%} expected anomaly rate configured")
        
        return assessment
    
    @staticmethod
    def _assess_data_impact(anomalies, df, method: str) -> list:
        """Assess the data impact of detected anomalies"""
        impact_assessment = []
        
        if not anomalies:
            impact_assessment.append("âœ… **POSITIVE DATA QUALITY**: No anomalies detected - data consistent with expected patterns")
            return impact_assessment
        
        anomaly_count = len(anomalies) if isinstance(anomalies, list) else 1
        
        # Scale-based impact assessment
        if df is not None:
            total_records = df.shape[0]
            impact_scale = (anomaly_count / total_records) * 100
            
            if impact_scale > 5:
                impact_assessment.append("ğŸš¨ **HIGH DATA IMPACT**: Significant statistical anomalies detected")
                impact_assessment.append("âš¡ **IMMEDIATE ANALYSIS**: Statistical investigation and validation required")
            elif impact_scale > 1:
                impact_assessment.append("âš ï¸ **MODERATE DATA IMPACT**: Notable statistical deviations present")
                impact_assessment.append("ğŸ“Š **TECHNICAL REVIEW**: Detailed statistical examination recommended")
            else:
                impact_assessment.append("ğŸ“‰ **LOW DATA IMPACT**: Isolated anomalies with minimal statistical effect")
                impact_assessment.append("ğŸ” **ROUTINE MONITORING**: Standard statistical verification applicable")
        
        # Data domain-specific impact analysis
        if df is not None:
            column_names = df.columns.tolist()
            column_text = " ".join(column_names).lower()
            
            # Financial/numeric data
            if any(keyword in column_text for keyword in ['revenue', 'cost', 'price', 'profit', 'sales']):
                impact_assessment.append("ğŸ’° **NUMERICAL DATA IMPACT**: Anomalies affect critical quantitative variables")
                impact_assessment.append("ğŸ¯ **PRIORITY**: Numerical anomalies require statistical validation")
            
            # Entity data
            if any(keyword in column_text for keyword in ['customer', 'user', 'satisfaction', 'service']):
                impact_assessment.append("ğŸ‘¥ **ENTITY DATA IMPACT**: Anomalies affect entity-related variables")
                impact_assessment.append("ï¿½ **SEGMENTATION**: Consider isolating affected data segments")
            
            # Process data
            if any(keyword in column_text for keyword in ['process', 'operation', 'efficiency', 'quality']):
                impact_assessment.append("âš™ï¸ **PROCESS DATA IMPACT**: Data quality in process metrics compromised")
                impact_assessment.append("ğŸ”§ **DATA REVIEW**: Evaluate data collection and processing procedures")
            
            # Compliance/integrity impact
            if any(keyword in column_text for keyword in ['compliance', 'regulation', 'audit', 'risk']):
                impact_assessment.append("âš–ï¸ **DATA INTEGRITY IMPACT**: Statistical reliability and validity implications")
                impact_assessment.append("ğŸ“‹ **DOCUMENTATION**: Record anomaly patterns for statistical analysis")
        
        # Method-specific data implications
        if 'fraud' in method.lower():
            impact_assessment.append("ğŸš¨ **FRAUD PATTERNS**: Statistical signatures of potentially fraudulent activity")
        elif 'quality' in method.lower():
            impact_assessment.append("âœ… **QUALITY METRICS**: Data quality deviations identified")
        elif 'security' in method.lower():
            impact_assessment.append("ğŸ›¡ï¸ **SECURITY ANOMALIES**: Statistical patterns indicate potential security concerns")
        
        return impact_assessment
    
    @staticmethod
    def _assess_risk_levels(anomalies, anomaly_scores, thresholds: dict) -> list:
        """Assess risk levels of detected anomalies"""
        risk_assessment = []
        
        if not anomalies:
            risk_assessment.append("âœ… **MINIMAL RISK**: No anomalies detected - risk levels within acceptable parameters")
            return risk_assessment
        
        # Score-based risk assessment
        if anomaly_scores and isinstance(anomaly_scores, list):
            scores_array = np.array(anomaly_scores)
            
            if len(scores_array) > 0:
                max_score = np.max(scores_array)
                mean_score = np.mean(scores_array)
                
                # Risk categorization based on scores
                if max_score > 0.8:
                    risk_assessment.append("ğŸš¨ **CRITICAL RISK**: Maximum anomaly score indicates severe deviation")
                elif max_score > 0.6:
                    risk_assessment.append("âš ï¸ **HIGH RISK**: Significant anomaly scores require attention")
                elif max_score > 0.4:
                    risk_assessment.append("ğŸ“Š **MODERATE RISK**: Noticeable anomaly patterns detected")
                else:
                    risk_assessment.append("ğŸ“‰ **LOW RISK**: Minor anomaly scores indicate minor deviations")
                
                # Distribution analysis
                high_risk_count = np.sum(scores_array > 0.7)
                medium_risk_count = np.sum((scores_array > 0.4) & (scores_array <= 0.7))
                low_risk_count = len(scores_array) - high_risk_count - medium_risk_count
                
                risk_assessment.append(f"ğŸ“Š **RISK DISTRIBUTION**: {high_risk_count} critical, {medium_risk_count} moderate, {low_risk_count} low risk")
        
        # Threshold-based assessment
        if thresholds:
            for threshold_name, threshold_value in thresholds.items():
                risk_assessment.append(f"ğŸ¯ **{threshold_name.upper()} THRESHOLD**: {threshold_value} - configured for risk detection")
        
        # Frequency-based risk
        anomaly_count = len(anomalies) if isinstance(anomalies, list) else 1
        
        if anomaly_count > 100:
            risk_assessment.append("ğŸš¨ **SYSTEMIC RISK**: Large number of anomalies indicates system-wide issues")
        elif anomaly_count > 20:
            risk_assessment.append("âš ï¸ **PATTERN RISK**: Multiple anomalies suggest recurring issues")
        elif anomaly_count > 5:
            risk_assessment.append("ğŸ“Š **CLUSTER RISK**: Several anomalies indicate localized problems")
        else:
            risk_assessment.append("ğŸ¯ **ISOLATED RISK**: Few anomalies suggest isolated incidents")
        
        # Business continuity risk
        risk_assessment.append("ğŸ“‹ **BUSINESS CONTINUITY**: Assess impact on ongoing operations and customer service")
        risk_assessment.append("ğŸ”„ **RECOVERY PLANNING**: Develop contingency plans for anomaly response")
        
        return risk_assessment
    
    @staticmethod
    def _generate_action_recommendations(anomalies, method: str) -> list:
        """Generate specific action recommendations based on anomalies"""
        recommendations = []
        
        if not anomalies:
            recommendations.extend([
                "âœ… **MAINTAIN MONITORING**: Continue current detection parameters",
                "ğŸ“Š **PERFORMANCE REVIEW**: Validate detection system effectiveness",
                "ğŸ”„ **ROUTINE MAINTENANCE**: Maintain standard operational procedures"
            ])
            return recommendations
        
        anomaly_count = len(anomalies) if isinstance(anomalies, list) else 1
        
        # Immediate actions
        recommendations.append("âš¡ **IMMEDIATE ACTIONS**:")
        
        if anomaly_count > 50:
            recommendations.extend([
                "   ğŸš¨ Implement comprehensive statistical analysis",
                "   ï¿½ Conduct multivariate outlier validation",
                "   ï¿½ Consider isolation of affected data points"
            ])
        elif anomaly_count > 10:
            recommendations.extend([
                "   âš ï¸ Perform detailed statistical analysis",
                "   ğŸ“Š Conduct root cause analysis using statistical methods",
                "   ğŸ” Investigate correlated variables and patterns"
            ])
        else:
            recommendations.extend([
                "   ğŸ” Examine individual anomalies statistically",
                "   ğŸ“‹ Document statistical properties of anomalies",
                "   ğŸ¯ Validate with appropriate statistical tests"
            ])
        
        # Investigation actions
        recommendations.append("ğŸ” **INVESTIGATION ACTIONS**:")
        recommendations.extend([
            "   ğŸ“Š Analyze anomaly distributions and statistical properties",
            "   ğŸ•’ Review temporal trends and autocorrelation patterns",
            "   ğŸ”— Check variable relationships and covariance structures",
            "   ï¿½ Apply multivariate analysis techniques"
        ])
        
        # Prevention actions
        recommendations.append("ğŸ›¡ï¸ **PREVENTION ACTIONS**:")
        recommendations.extend([
            "   ğŸ”§ Optimize statistical detection thresholds",
            "   ğŸ“ˆ Enhance monitoring with additional statistical metrics",
            "   ğŸ“‹ Update data validation procedures",
            "   ğŸ“Š Implement improved outlier detection algorithms"
        ])
        
        # Method-specific recommendations
        if 'statistical' in method.lower():
            recommendations.extend([
                "ğŸ“Š **STATISTICAL ACTIONS**:",
                "   ğŸ“ˆ Validate statistical assumptions and parameters",
                "   ğŸ”„ Consider robust statistical methods for outliers"
            ])
        elif 'machine_learning' in method.lower() or 'isolation' in method.lower():
            recommendations.extend([
                "ğŸ¤– **ML MODEL ACTIONS**:",
                "   ğŸ”„ Retrain model with recent data",
                "   ğŸ¯ Tune hyperparameters for better precision"
            ])
        
        # Communication actions
        recommendations.append("ğŸ“¢ **TECHNICAL DOCUMENTATION**:")
        recommendations.extend([
            "   ğŸ“‹ Create statistical summary of anomaly findings",
            "   ï¿½ Generate visualization of anomaly distributions",
            "   ï¿½ Document methodological approach to anomaly detection",
            "   ğŸ“ Record technical limitations and statistical assumptions"
        ])
        
        # Follow-up actions
        recommendations.append("ğŸ”„ **FOLLOW-UP ACTIONS**:")
        recommendations.extend([
            "   ğŸ“… Implement periodic statistical quality checks",
            "   ğŸ“Š Monitor statistical properties of variables over time",
            "   ğŸ¯ Measure effectiveness of anomaly detection methods",
            "   ğŸ“ˆ Update detection models with new data distributions"
        ])
        
        return recommendations
